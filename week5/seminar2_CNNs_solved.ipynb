{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 1D CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Самописная 1D конволюция\n",
    "##### Пройдемся фильтром длины 3 по одномерному массиву:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_len = 6\n",
    "image = [i for i in range(image_len)]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 0, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_1d = [-1, 0, 2]\n",
    "filter_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 7]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_filtered = []\n",
    "for i in range(image_len-2):\n",
    "    val = 0\n",
    "    for j in range(3):\n",
    "        pass\n",
    "        ### YOUR CODE HERE\n",
    "        val += image[i+j] * filter_1d[j]\n",
    "        \n",
    "    image_filtered.append(val)\n",
    "image_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### То же самое на PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor = torch.arange(6).float()\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(1, 1, kernel_size=(3,), stride=(1,), bias=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# объявляем конволюцию\n",
    "conv1d_1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "conv1d_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight [1, 1, 3], but got 1-dimensional input of size [6] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-57c38298ed88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# так работать не будет, см. след. ячейку\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconv1d_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    206\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    207\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 208\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight [1, 1, 3], but got 1-dimensional input of size [6] instead"
     ]
    }
   ],
   "source": [
    "# так работать не будет, см. след. ячейку\n",
    "conv1d_1(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2667, -0.9517, -1.6366, -2.3216]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Модуль ожидает, что на вход придет батч картинок. \n",
    "Поэтому первое измерение входного тензора - размер батча (в нашем случае = 1).\n",
    "Второе измерение - количество каналов (например, в картинках часто бывает 3 канала - RGB).  \n",
    "В нашем случае, количество каналов = 1\n",
    "\"\"\" \n",
    "conv1d_1(image_tensor.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[-0.4238, -0.2558, -0.0054]]], requires_grad=True),\n",
       " torch.Size([1, 1, 3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_1.weight# Мы получили не те значения, что и в прошлом упражнении. Попробуем разобраться и посмотрим на наш фильтр:\n",
    "conv1d_1.weight, conv1d_1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-1.,  0.,  2.]]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Давайте заменим этот фильтр на фильтр из прошлого упражнения:\n",
    "conv1d_1.weight = nn.Parameter(torch.Tensor([-1, 0, 2]).unsqueeze(0).unsqueeze(0))\n",
    "conv1d_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 5., 6., 7.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Мы получили те же значения, что и в прошлом упражнении:\n",
    "conv1d_1(image_tensor.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обратите внимание, что размер выходного вектора - 4, а у исходного был 6.\n",
    "# Чтобы размер сохранялся, нужно использовать padding: \n",
    "#    исходный вектор будет расширяться с двух сторон на заданную длину\n",
    "#    Укажите в объявлении конволюции параметр padding=1\n",
    "conv1d_2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "conv1d_2.weight.data = conv1d_1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.,  4.,  6.,  8., 10.,  0.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# длина выходного вектора теперь тоже равна 6\n",
    "conv1d_2(image_tensor.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 2D CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "image_tensor_2d = torch.arange(9).reshape(3,3).float()\n",
    "print(image_tensor_2d.shape)\n",
    "print(image_tensor_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2, 2])\n",
      "tensor([[[[0., 1.],\n",
      "          [2., 3.]]]])\n"
     ]
    }
   ],
   "source": [
    "filter_2d = torch.arange(4).reshape(2,2).float().unsqueeze(0).unsqueeze(0)\n",
    "print(filter_2d.shape)\n",
    "print(filter_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.1825, -0.3867],\n",
       "          [-0.0208, -0.3883]]]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Объявляем 2D-конволюцию\n",
    "conv2d_1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=0, bias=False)\n",
    "conv2d_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[0., 1.],\n",
       "          [2., 3.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_1.weight = nn.Parameter(filter_2d)\n",
    "conv2d_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[19., 25.],\n",
       "          [37., 43.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_1(image_tensor_2d.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  3.,  8.,  4.],\n",
       "          [ 9., 19., 25., 10.],\n",
       "          [21., 37., 43., 16.],\n",
       "          [ 6.,  7.,  8.,  0.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Добавляем padding\n",
    "conv2d_2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=1, bias=False)\n",
    "conv2d_2.weight = conv2d_1.weight\n",
    "conv2d_2(image_tensor_2d.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.,  8.],\n",
       "          [19., 25.],\n",
       "          [37., 43.],\n",
       "          [ 7.,  8.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Играемся с параметром padding\n",
    "conv2d_3 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=(1, 0), bias=False)\n",
    "conv2d_3.weight = conv2d_1.weight\n",
    "conv2d_3(image_tensor_2d.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 9., 19., 25., 10.],\n",
       "          [21., 37., 43., 16.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Играемся с параметром padding\n",
    "conv2d_4 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=(0, 1), bias=False)\n",
    "conv2d_4.weight = conv2d_1.weight\n",
    "conv2d_4(image_tensor_2d.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 2., 0.],\n",
       "          [0., 3., 4., 5., 0.],\n",
       "          [0., 6., 7., 8., 0.]]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Можно сначала падить через функцию, а затем подавать тензор в конволюционный слой\n",
    "F.pad(image_tensor_2d.unsqueeze(0).unsqueeze(0), (1,1,0,0), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 2.],\n",
       "          [0., 3., 4., 5.],\n",
       "          [0., 6., 7., 8.]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Этот способ более гибкий, потому что позволяет падить только с одной выбранной стороны:\n",
    "F.pad(image_tensor_2d.unsqueeze(0).unsqueeze(0), (1,0,0,0), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[19., 25.],\n",
       "          [37., 43.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "# Вспомним выход нашей первой 2D конволюции\n",
    "conv2d_1(image_tensor_2d.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0653], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### За что же отвечает параметр bias?\n",
    "conv2d_5 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=0, bias=True)\n",
    "conv2d_5.weight = conv2d_1.weight\n",
    "conv2d_5.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сделаем bias равным единице:\n",
    "conv2d_5.bias = nn.Parameter(torch.ones(1))\n",
    "conv2d_5.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[19., 25.],\n",
      "          [37., 43.]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "tensor([[[[20., 26.],\n",
      "          [38., 44.]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "# вспомним выход из нашей первой конволюции, и сравним его с выходом с учетом bias:\n",
    "print(conv2d_1(image_tensor_2d.unsqueeze(0).unsqueeze(0)))\n",
    "print(conv2d_5(image_tensor_2d.unsqueeze(0).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Кстати, наш фильтр не обязательно будет квадратным:\n",
    "conv2d_6 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,3), stride=1, padding=0, bias=False)\n",
    "conv2d_6.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[0., 1., 2.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_6.weight = nn.Parameter(torch.arange(3).float().reshape(1,1,1,3))\n",
    "conv2d_6.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[2.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пробегаться по картинке можно и фильтром 1x1:\n",
    "conv2d_7 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "conv2d_7.weight = nn.Parameter(torch.ones(1, 1, 1, 1)*2)\n",
    "conv2d_7.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  2.,  4.],\n",
       "          [ 6.,  8., 10.],\n",
       "          [12., 14., 16.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_7(image_tensor_2d.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[2.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# А сейчас посмотрим на stride в действии:\n",
    "conv2d_8 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1, stride=2, padding=0, bias=False)\n",
    "conv2d_8.weight = nn.Parameter(torch.ones(1, 1, 1, 1)*2)\n",
    "conv2d_8.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  4.],\n",
       "          [12., 16.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_8(image_tensor_2d.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  2.,  4.],\n",
       "          [12., 14., 16.]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Как и с остальными параметрами, stride может быть разным по разным осям:\n",
    "conv2d_9 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=1, stride=(2, 1), padding=0, bias=False)\n",
    "conv2d_9.weight = nn.Parameter(torch.ones(1, 1, 1, 1)*2)\n",
    "conv2d_9(image_tensor_2d.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Let's work with pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './lecun.jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прочитайте изображение в grayscale и rgb (cv2.imread, флаг 0 - grayscale)\n",
    "img_grayscale = cv2.imread(path, 0) # 0 означет grayscale\n",
    "img_rgb = cv2.imread(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert img_grayscale.shape == (3960, 2640)\n",
    "assert img_rgb.shape == (3960, 2640, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, cmap=None, figsize=(16, 9)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to see the grayscale image\n",
    "# show_image(img_grayscale, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to see the change in the picture\n",
    "# синий и красный каналы перепутаны в cv2\n",
    "#show_image(img_rgb[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция, которая переводит изображение в тензор\n",
    "to_tensor = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_grayscale = to_tensor(img_grayscale)\n",
    "tensor_rgb = to_tensor(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3960, 2640])\n",
      "torch.Size([3, 3960, 2640])\n"
     ]
    }
   ],
   "source": [
    "# pytorch хранит канал в нулевой размерности\n",
    "print(tensor_grayscale.shape)\n",
    "print(tensor_rgb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_grayscale = tensor_grayscale.reshape([1, 1, 3960, 2640])\n",
    "tensor_rgb = tensor_rgb.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tensor_grayscale.shape == torch.Size([1, 1, 3960, 2640])\n",
    "assert tensor_rgb.shape == torch.Size([1, 3, 3960, 2640])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим свертку, которая размоет чёрно-белое изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 50\n",
    "layer_grayscale = nn.Conv2d(1, 1, kernel_size=kernel_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "layer_grayscale.weight.data = torch.ones_like(layer_grayscale.weight.data)\n",
    "layer_grayscale.weight.data /= torch.sum(layer_grayscale.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0004],\n",
       "          [0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0004],\n",
       "          [0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0004],\n",
       "          ...,\n",
       "          [0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0004],\n",
       "          [0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0004],\n",
       "          [0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0004]]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_grayscale.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(50, 50), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_grayscale.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor_grayscale = layer_grayscale(tensor_grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3911, 2591])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_grayscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция, переводящая тензор в PIL-изображение\n",
    "to_pil_image = torchvision.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный размер изображения: torch.Size([1, 1, 3911, 2591])\n",
      "Сжатие всех измерений: torch.Size([3911, 2591])\n",
      "Сжатие измерения 0: torch.Size([1, 3911, 2591])\n",
      "Сжатие измерения 1: torch.Size([1, 3911, 2591])\n",
      "Сжатие измерения 2: torch.Size([1, 1, 3911, 2591])\n",
      "Сжатие измерения 3: torch.Size([1, 1, 3911, 2591])\n"
     ]
    }
   ],
   "source": [
    "print(\"Исходный размер изображения:\", output_tensor_grayscale.shape)\n",
    "print(\"Сжатие всех измерений:\", output_tensor_grayscale.squeeze().shape)\n",
    "print(\"Сжатие измерения 0:\", output_tensor_grayscale.squeeze(0).shape)\n",
    "print(\"Сжатие измерения 1:\", output_tensor_grayscale.squeeze(1).shape)\n",
    "print(\"Сжатие измерения 2:\", output_tensor_grayscale.squeeze(2).shape)\n",
    "print(\"Сжатие измерения 3:\", output_tensor_grayscale.squeeze(3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_img_grayscale = output_tensor_grayscale.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blurred image (uncomment and run to see)\n",
    "# to_pil_image(output_img_grayscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим свертку, которая размоет цветное изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 50\n",
    "layer_rgb = nn.Conv2d(3, 3, kernel_size=kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 50, 50])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (out_channels, in_channels, size, size)\n",
    "layer_rgb.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_rgb.weight.data = torch.zeros_like(layer_rgb.weight.data)\n",
    "\n",
    "for i in range(3):\n",
    "    layer_rgb.weight.data[i][i] = torch.ones_like(layer_rgb.weight.data[i][i])\n",
    "    layer_rgb.weight.data[i][i] /= torch.sum(layer_rgb.weight.data[i][i])\n",
    "\n",
    "layer_rgb.bias.data = torch.zeros_like(layer_rgb.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_rgb.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 3, kernel_size=(50, 50), stride=(1, 1))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_rgb.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor_rgb = layer_rgb(tensor_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3911, 2591])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_img_rgb = output_tensor_rgb.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to see\n",
    "#to_pil_image(output_img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# синий и красный каналы перепутаны\n",
    "channel_0 = output_img_rgb[0:1, :, :].clone()\n",
    "channel_2 = output_img_rgb[2:3, :, :].clone()\n",
    "\n",
    "output_img_rgb[0:1, :, :] = channel_2\n",
    "output_img_rgb[2:3, :, :] = channel_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to see\n",
    "# to_pil_image(output_img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на границы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "layer_vertical_border = nn.Conv2d(1, 1, kernel_size=kernel_size)\n",
    "layer_horizontal_border = nn.Conv2d(1, 1, kernel_size=kernel_size)\n",
    "layer_full_border = nn.Conv2d(1, 1, kernel_size=kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(layer_vertical_border.weight.data.shape)\n",
    "print(layer_horizontal_border.weight.data.shape)\n",
    "print(layer_full_border.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1,  0,  1],\n",
       "        [-2,  0,  2],\n",
       "        [-1,  0,  1]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_vertical_border = torch.tensor([-1, 0, 1, -2, 0, 2, -1, 0, 1]).reshape([3, 3])\n",
    "conv_vertical_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -2, -1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 1,  2,  1]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_horizontal_border = torch.tensor([-1, -2, -1, 0, 0, 0, 1, 2, 1]).reshape([3, 3])\n",
    "conv_horizontal_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -1, -1],\n",
       "        [-1,  8, -1],\n",
       "        [-1, -1, -1]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_full_border = torch.tensor([-1, -1, -1, -1, 8, -1, -1, -1, -1]).reshape([3, 3])\n",
    "conv_full_border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_vertical_border.weight.data[0][0] = conv_vertical_border\n",
    "layer_horizontal_border.weight.data[0][0] = conv_horizontal_border\n",
    "layer_full_border.weight.data[0][0] = conv_full_border\n",
    "\n",
    "layer_vertical_border.bias.data = torch.zeros_like(layer_vertical_border.bias.data)\n",
    "layer_horizontal_border.bias.data = torch.zeros_like(layer_horizontal_border.bias.data)\n",
    "layer_full_border.bias.data = torch.zeros_like(layer_full_border.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(layer_vertical_border.weight.data))\n",
    "print(torch.sum(layer_horizontal_border.weight.data))\n",
    "print(torch.sum(layer_full_border.weight.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.,  0.,  1.],\n",
      "          [-2.,  0.,  2.],\n",
      "          [-1.,  0.,  1.]]]])\n",
      "\n",
      "tensor([[[[-1., -2., -1.],\n",
      "          [ 0.,  0.,  0.],\n",
      "          [ 1.,  2.,  1.]]]])\n",
      "\n",
      "tensor([[[[-1., -1., -1.],\n",
      "          [-1.,  8., -1.],\n",
      "          [-1., -1., -1.]]]])\n"
     ]
    }
   ],
   "source": [
    "print(layer_vertical_border.weight.data)\n",
    "print()\n",
    "print(layer_horizontal_border.weight.data)\n",
    "print()\n",
    "print(layer_full_border.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_vertical_border.eval()\n",
    "layer_horizontal_border.eval()\n",
    "layer_full_border.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tensor_vertical_border = layer_vertical_border(tensor_grayscale)\n",
    "output_tensor_horizontal_border = layer_horizontal_border(tensor_grayscale)\n",
    "output_tensor_full_border = layer_full_border(tensor_grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3958, 2638])\n",
      "torch.Size([1, 1, 3958, 2638])\n",
      "torch.Size([1, 1, 3958, 2638])\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor_vertical_border.shape)\n",
    "print(output_tensor_horizontal_border.shape)\n",
    "print(output_tensor_full_border.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_img_vertical_border = output_tensor_vertical_border.squeeze(0)\n",
    "output_img_horizontal_border = output_tensor_horizontal_border.squeeze(0)\n",
    "output_img_full_border = output_tensor_full_border.squeeze(0)\n",
    "\n",
    "output_img_vertical_border[output_img_vertical_border < 0] = 0\n",
    "output_img_vertical_border[output_img_vertical_border > 1] = 1\n",
    "\n",
    "output_img_horizontal_border[output_img_horizontal_border < 0] = 0\n",
    "output_img_horizontal_border[output_img_horizontal_border > 1] = 1\n",
    "\n",
    "output_img_full_border[output_img_full_border < 0] = 0\n",
    "output_img_full_border[output_img_full_border > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to see\n",
    "# to_pil_image(img_grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to see\n",
    "# to_pil_image(output_img_vertical_border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#uncomment to see\n",
    "# to_pil_image(output_img_horizontal_border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#uncomment to see\n",
    "# to_pil_image(output_img_full_border)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) MNIST  Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x7F2C2C2946A0>, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F2C2C294F60>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNIST - подкласс torch.utils.data.Dataset, и поддерживает __getitem__(), \n",
    "#     т.е. можно получать элементы по индексу, как в листе:\n",
    "train_dataset = torchvision.datasets.MNIST(root='.', download=True)\n",
    "print(train_dataset[0])\n",
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
       "           -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
       "            2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
       "            2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
       "            2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "            2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
       "            0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "            2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
       "           -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
       "           -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
       "            0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
       "            2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
       "            2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
       "            2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
       "            2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
       "            2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
       "            1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
       "            0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
       "            2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
       "            2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Чтобы датасет возвращал тензоры, а не  PIL.Image объекты, добавим трансформации\n",
    "# Нормализуем данные для более легкого обучения модели (mean, std посчитаны)\n",
    "transform=transforms.Compose([\n",
    "   transforms.ToTensor(),\n",
    "   transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_dataset = torchvision.datasets.MNIST(root='.', transform=transform, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST(root='.', transform=transform, train=False, download=True)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = random.randint(0, len(train_dataset)-1)\n",
    "plt.imshow(train_dataset[ind][0].squeeze(0))\n",
    "print('Digit:', train_dataset[ind][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные из датасета в модель будет отправлять Dataloader.\n",
    "# Он отвечает за сэмплирование данных, чтение (в т.ч. параллельное) с диска и собирание данных в батчи\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, stride=2, padding=0, bias=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, stride=2, padding=0, bias=True)\n",
    "        self.linear1 = nn.Linear(360, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(-1, 360)\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.317588\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.292395\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.278966\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 2.281613\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 2.275034\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.274742\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.234548\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.238369\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.184466\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.178064\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.068908\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.972679\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.747774\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.546985\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.139620\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.849667\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.668234\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.406319\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.596697\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.488231\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = CNNModel().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 4\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "                100. * batch_idx / len(train_dataloader), loss.item()))\n",
    "    ### YOUR CODE HERE:\n",
    "    # print test set accuracy after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: 5, Predicted: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind_test = random.randint(0, len(test_dataset)-1)\n",
    "img, true = test_dataset[ind_test]\n",
    "preds = model(img.unsqueeze(0).to(device))\n",
    "pred = torch.argmax(preds).item()\n",
    "plt.imshow(img.squeeze(0))\n",
    "print('True: {}, Predicted: {}'.format(true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1597159819284"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}